{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf1be83-f297-4de8-ad19-d88a6e2e47b5",
   "metadata": {},
   "source": [
    "# Анализ датасетов, сгенерированных моделью Llama 13B, с помощью baseline-классификаторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97477742-ba32-469e-aae3-c897d2e9e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME_PREFIX = 'llama_essays'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ecb18da-1111-4dac-a03e-cf0517b9ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaba4137-e4e6-4d65-ab1b-1c3b7114bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/human_essays.csv\", encoding=\"utf-8\").assign(label=0)[[\"text\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bfc91a3-13e2-40da-b5fb-72a13eddfe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(orig_df: pd.DataFrame, filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Загружает датасет из CSV, добавляет метку и объединяет с исходным DataFrame. Затем перемешивает строки.\n",
    "\n",
    "    :param orig_df: исходный DataFrame\n",
    "    :param filename: путь к CSV-файлу\n",
    "    :return: объединённый и перемешанный DataFrame\n",
    "    \"\"\"\n",
    "    new_data = pd.read_csv(filename, encoding=\"utf-8\").assign(label=1)[[\"text\", \"label\"]]\n",
    "\n",
    "    result = pd.concat([orig_df, new_data], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "515c723c-41a3-467f-bb27-e341c6b0ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    get_dataset(df, f\"../datasets/{FILE_NAME_PREFIX}_{i}.csv\")\n",
    "    for i in range(1, 6)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38f9f073-8681-415d-8dbe-6b2e5ac86432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Directed by the American film producer and dir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Table of Contents\\n 1. “The Jewelry” by Guy de...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABC Company, a leading player in the retail in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The site acknowledges that COVID-19 is here to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Summary of Hitchen’s Main Arguments\\n\\nHit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Table of Contents\\n 1. Introduction\\n 2. Histo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>In \"The Hero with a Thousand Faces,\" psychoana...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Introduction\\n\\nOperational Management is defi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Introduction\\n\\nWater is a kind of chemical su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Introduction\\n\\nJail overcrowding is a situati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Directed by the American film producer and dir...      0\n",
       "1  Table of Contents\\n 1. “The Jewelry” by Guy de...      0\n",
       "2  ABC Company, a leading player in the retail in...      1\n",
       "3  The site acknowledges that COVID-19 is here to...      0\n",
       "4  The Summary of Hitchen’s Main Arguments\\n\\nHit...      0\n",
       "5  Table of Contents\\n 1. Introduction\\n 2. Histo...      0\n",
       "6  In \"The Hero with a Thousand Faces,\" psychoana...      1\n",
       "7  Introduction\\n\\nOperational Management is defi...      0\n",
       "8  Introduction\\n\\nWater is a kind of chemical su...      0\n",
       "9  Introduction\\n\\nJail overcrowding is a situati...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb7fe3-d650-4fd4-bd91-da5c2ae20d78",
   "metadata": {},
   "source": [
    "### LogisticRegression + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1f9bc29-7204-4a95-a998-59fabc9da4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_classification_report(data):\n",
    "    \"\"\"\n",
    "    Обучает Logistic Regression на TF-IDF признаках и возвращает classification report и F1-score.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data[\"text\"],\n",
    "        data[\"label\"],\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=data[\"label\"],\n",
    "    )\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=(1, 2),\n",
    "    )\n",
    "\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_vec, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "\n",
    "    return classification_report(y_test, y_pred), f1_score(y_test, y_pred, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aa76a78-0a67-49f2-9991-f6229cad567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_results = []\n",
    "for df in datasets:\n",
    "    log_results.append(get_log_classification_report(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14075836-58d6-4149-8ccc-08282a524427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отчёт по датасету №1\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     25573\n",
      "           1       1.00      0.46      0.63       200\n",
      "\n",
      "    accuracy                           1.00     25773\n",
      "   macro avg       1.00      0.73      0.81     25773\n",
      "weighted avg       1.00      1.00      1.00     25773\n",
      "\n",
      "F1-score: 0.6301\n",
      "\n",
      "\n",
      "Отчёт по датасету №2\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     25573\n",
      "           1       1.00      0.62      0.77       200\n",
      "\n",
      "    accuracy                           1.00     25773\n",
      "   macro avg       1.00      0.81      0.88     25773\n",
      "weighted avg       1.00      1.00      1.00     25773\n",
      "\n",
      "F1-score: 0.7654\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (report, f1) in enumerate(log_results, start=1):\n",
    "    print(f\"Отчёт по датасету №{i}\\n\")\n",
    "    print(report)\n",
    "    print(f\"F1-score: {f1:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6dcaf1-758c-41b1-9b89-7de022655f3e",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efefea0-0cb6-44c3-8325-ec5f0213ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boost_classification_report(data):\n",
    "    \"\"\"\n",
    "    Обучает Word2Vec + XGBoost и возвращает classification report и F1-score.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "\n",
    "    df[\"tokens\"] = df[\"text\"].apply(lambda text: simple_preprocess(str(text)))\n",
    "\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=df[\"tokens\"],\n",
    "        vector_size=300,\n",
    "        window=7,\n",
    "        min_count=2,\n",
    "        workers=12,\n",
    "        sg=1,\n",
    "    )\n",
    "\n",
    "    def text_to_vector(tokens, model):\n",
    "        vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "        return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "    X = np.vstack([text_to_vector(tokens, w2v_model) for tokens in df[\"tokens\"]])\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y,\n",
    "    )\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.5,\n",
    "        eval_metric=\"logloss\",\n",
    "        n_jobs=12,\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    return classification_report(y_test, y_pred), f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78bf3ea-3bda-4b5c-b0d0-98dd2b04c2cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boost_results = []\n",
    "for df in datasets:\n",
    "    boost_results.append(get_boost_classification_report(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545975ee-fe29-4c2b-8ba1-49e92f4113eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (report, f1) in enumerate(boost_results, start=1):\n",
    "    print(f\"Отчёт по датасету №{i}\\n\")\n",
    "    print(report)\n",
    "    print(f\"F1-score: {f1:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101a1907-c126-4431-898d-2b6222ebf410",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad19163-74aa-46e7-bb3e-925c3c6d6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6bf5a-9a63-487a-9217-3001a5cc634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb2d1c-0098-4a54-a8d7-ee63c4d16b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Токенизирует тексты одним батчем для ускорения обучения.\n",
    "    Выполняет усечение до max_length и padding до одинаковой длины,\n",
    "    возвращая тензоры PyTorch (input_ids, attention_mask).\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_bert_classification_report(data, model_name=\"bert-base-uncased\", max_length=512, batch_size=8, epochs=1):\n",
    "    \"\"\"\n",
    "    Fine-tuning BERT для бинарной классификации текста.\n",
    "    Возвращает classification report и F1-score.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data[\"text\"],\n",
    "        data[\"label\"],\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=data[\"label\"],\n",
    "    )\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    train_enc = tokenize_texts(X_train, tokenizer, max_length)\n",
    "    test_enc = tokenize_texts(X_test, tokenizer, max_length)\n",
    "\n",
    "    train_dataset = TokenizedDataset(train_enc, y_train)\n",
    "    test_dataset = TokenizedDataset(test_enc, y_test)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=data[\"label\"].nunique(),\n",
    "    )\n",
    "    model = torch.compile(model)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            outputs.loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "            )\n",
    "            preds.extend(outputs.logits.argmax(dim=1).cpu().numpy())\n",
    "            labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    return classification_report(labels, preds), f1_score(labels, preds, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f3c26-ac4a-4c59-9420-0caf6c158919",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results = []\n",
    "for df in datasets:\n",
    "    bert_results.append(get_bert_classification_report(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5911ca7-0205-4b75-92c4-a3053abc4d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (report, f1) in enumerate(bert_results, start=1):\n",
    "    print(f\"Отчёт по датасету №{i}\\n\")\n",
    "    print(report)\n",
    "    print(f\"F1-score: {f1:.4f}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (M2 GPU)",
   "language": "python",
   "name": "m2gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
